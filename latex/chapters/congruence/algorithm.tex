\section*{Algorithms}
\label{sec:algorithm}
\SetKwInput{KwGlobal}{Global}
\SetKw{KwInvariant}{Invariant}
\SetKw{KwAssert}{Assert}

%\newfloat{algorithm}{t}{lop}

In this section, we will present a congruence closure algorithm that is able to produce explanations.
The algorithm is a mix of the approaches of the algorithms presented in \cite{Fontaine2004} and \cite{Nieuwenhuis2005,Nieuwenhuis2007}.
The basic structure of the algorithm is inherited from \cite{Fontaine2004}, which itself inherits its structure from the algorithm of Nelson and Oppen \cite{Nelson1980}.
The technique to store and deduce equations of non constant terms is inspired from \cite{Nieuwenhuis2005,Nieuwenhuis2007}.
Additionally the proof forest structure described below, was proposed by \cite{Nieuwenhuis2005,Nieuwenhuis2007}.

\subsection*{Preliminaries}

Our congruence closure algorithm operates on curried terms.
Curried terms use a single binary function symbol to represent general terms.
More formally let $\mathcal{F}$ be a finite set of functions with a designated binary function symbol $f \in \mathcal{F}$ and let every other function symbol in $\mathcal{F}$ be a constant.
A term w.r.t. a signature of this form is called a \emph{curried term}.

It is possible to uniquely translate a general set of terms $\mathcal{T}^{\Sigma}$ with signature $\Sigma = \langle \mathcal{F},arity \rangle$ into a set of curried terms $\mathcal{T'}^{\Sigma'}$.
$\Sigma'$ is obtained from $\Sigma$ by setting $arity$ to zero for every function symbol in $\mathcal{F}$ and introducing the designated binary function symbol $f$ to $\mathcal{F}$.
The translation of a term $t \in \mathcal{T}^{\Sigma}$ is given in terms of the function $curry$.

$$
curry(t) = \Big\{
\begin{array}{ll}
	t & \text{ if } t \text{ is a constant }\\
	f(\ldots (f(f(g,curry(t_1)),curry(t_2)))\ldots,curry(t_n)) &\text{ if } t = g(t_1,\ldots, t_n)
\end{array}
$$

The idea of currying was introduced by M. Sch\"onfinkel \cite{Schoenfinkel1924} in 1924 and independently by Haskell B. Curry \cite{Curry1958} in 1958, who also lends his name to the concept.
Currying is not restricted to terms.
The general indea is to translate functions of type $A \times B \rightarrow C$ into functions of type $A \rightarrow B \rightarrow C$.
There is a close relation between currying and lambda calculus \cite{Church1936}.
Lambda calculus uses a single binary function $\lambda$.
Its arguments can either be elements of some set or again lambda terms.
For an introduction to lambda calculus, including currying in terms of lambda calculus and its relation to functional programming, see \cite{Barendregt1997}.

The benefit of working with curried terms is an easier and cleaner congruence closure algorithm that runs in optimal time $O(n \log(n))$.

Recently so called abstract congruence closure algorithms have been proposed and shown to be more efficient than traditional approaches \cite{Bachmair2000}.
The idea of abstract congruence closure is to introduce new constants for non constant terms.
Doing so, all of equations the algorithm has to take into account are of the form $c = d$ and $c = f(a,b)$, where $a,b,c,d$ are constants.
This replaces tedious preprocessing steps, for example transformation to a graph of outdegree 2 \cite{Downey1980},that are necessary for other algorithms to achieve the optimal running time.

Our method is does not employ the idea of abstract congruence closure.
We found that using currying is enough to obtain an algorithm with optimal running time and no tedious preprocessing steps.
The reason why we did not go for abstract congruence closure is, that we do not want to have the overhead of introducing and eliminating fresh constants.
In the context of proof compression, our congruence closure algorithm will be applied to relatively small instances very often.
We could introduce the extra constants for the whole proof before processing, but would still have to remove them from explanations every time we produce a new subproof.
It would be interesting to investigate, whether our intuition in that regard is right, or if it pays off to deal with extra constants.

Comming back to the explanation producing congruence closure algorithms that inspired ours, \cite{Nieuwenhuis2005,Nieuwenhuis2007} describes an abstract one using currying.
\cite{Fontaine2004} uses a traditional algorithm without currying and extra constants.
Our algorithm is a middle ground between them.

\subsection*{Congruence structure}

We call the underlying data structure of our congruence closure algorithm a \emph{congruence structure}.
A congruence structure for set of terms $\mathcal{T}$ is a collection of the following data structures.

\begin{itemize}
	\item Representative $r: \mathcal{T} \rightarrow \mathcal{T}$
	\item Congruence class $[.]: \mathcal{T} \rightarrow 2^\mathcal{T}$
	\item Left neighbors $lN: \mathcal{T} \rightarrow 2^\mathcal{T}$
	\item Right neighbors $rN: \mathcal{T} \rightarrow 2^\mathcal{T}$
	\item Lookup table $l: \mathcal{T} \times \mathcal{T} \rightarrow \mathcal{T}$
	\item Congruence graph $g$
	\item Queue $\mathcal{Q}$ of type $\mathcal{T} \times \mathcal{T}$
	\item Current explanations $\mathcal{M}: \mathcal{T} \times \mathcal{T} \rightarrow \mathcal{E}$
\end{itemize}

The representative is one particular term of a class of congruent terms.
It is used to identify whether two terms are already in the same congruence class and the data structures used for detecting equalities derived from the congruence axiom are kept updated only for representatives.
The congruence class structure represents a set of pairwise congruent terms.
It is used to keep track which representatives have to be updated when merging the classes of two terms.
The structures left neighbor and right neighbor for every term keep track of other terms that appear as the second argument in a compound term of the form $f(a,b)$.
The lookup table is used to keep track of all compound terms in the congruence structure and to merge compound terms, which arguments are congruent.
The congruence graph stores the derived equalities in a structured way, that allows to create explanations for a given pair of terms.
Edges are added to the graph in a lazy way, meaning that they are buffered and only actually entered into the graph when demanded.
The queue $\mathcal{Q}$ keeps track of the order in which edges should be added to the graph.
The function $\mathcal{M}$ stores the explanation for a buffered edge.
The main idea about buffering is to overwrite explanations, when an edge was added due to the congruence axiom, while it is entered as an input equation later on.
We call the unique congruence structure for $\mathcal{T} = \emptyset$ the \emph{empty congruence structure}.
It is not by coincidence that many of the used data structures are described as functions.
In fact our congruence closure algorithm can and is implemented in a functional way and the data structures can be implemented immutable.

\FloatBarrier

\subsection*{Congruence closure algorithms}

In this section we present the pseudocode of our congruence closure algorithm, state and prove its properties.
Most importantly we show that it the method is sound and complete and has optimal running time $O(n \log(n))$.
Computing the congruence closure of some set of equations $E$ is done by adding all of them to an ever growing congruence structure, which initially is empty.
Most algorithm pseudocodes do not include a return statement.
In fact every algorithm implicitly returns a (modified) congruence structure or simply modifies a global variable, which is the current congruence structure.
Since this has to be done in some order, we will often assume that $E$ is given as a sequence of equations rather than a set.
Adding an equation to a congruence structure is done with the \textt{addEquation} method.
The method adds boths sides of the equation to the current set of terms $\mathcal{T}$ using the \textt{addNode} method and afterwards merges the classes of the two terms.
The \texttt{addNode} method enlarges the set of terms and searches for equalities that are due to the congruence axiom.
The updates of $\mathcal{T}$ are not outlined explicitly, but are understood to happen implicitly.
The method \texttt{merge} initializes and guides the merging of terms.
The actual merging is done by the method \textt{union} by modifying the data structures.

\input{chapters/congruence/algorithms/addequation}

\input{chapters/congruence/algorithms/addnode}

\input{chapters/congruence/algorithms/merge}

\input{chapters/congruence/algorithms/lazyinsert}

\input{chapters/congruence/algorithms/lazyupdate}

\input{chapters/congruence/algorithms/union}

\begin{invariant}[Class]

For every $s \in \mathcal{T}$ and every $t \in [r(s)]$, $r(t) = r(s)$.

\label{invar:class}
\end{invariant}

\begin{proof}

Clearly the invariant is true when intializing $[s]$ in line \ref{initclass} of \textt{addNode}.

The only other point in the code that changes $[s]$ is line \ref{changeclass} of union.
Suppose the class of $u$ is enlarged by the class of $v$ in union and suppose the invariant holds before the union for those terms.
Before the update of $[r(u)]$ the representative of every term in $[r(v)]$ is set to $r(u)$.
Therefore the invariant remains valid after the update.

\end{proof}

\begin{invariant}[Lookup]

The lookup structure $l$ is defined for a pair of terms $(s,t)$ if and only if there is a term $f(a,b) \in \mathcal{T}$ such that $r(a) = r(s)$ and $r(b) = r(t)$.
\end{invariant}

\begin{proof}

Suppose $l$ is defined for some pair of terms $(s,t)$.
The value of $l(s,t)$ was either set in lines \ref{changel1} or \ref{changel2} of union or in line \ref{initl} of \textt{addNode}.
In the latter case, $l$ is set to $f(a,b)$ for the tuple $(r(a),r(b))$ and therefore the invariant holds at this point.
For changes to $r(a)$ or $r(b)$ in union the one implication of the invariant remains valid in case $l$ is defined for the new representatives, or $l$ is set for an additional pair of terms in lines \ref{changel1} or \ref{changel2}.
In case $l$ is set to $(new\_left,r(u))$ or $(r(u),new\_right)$ in union, there is an $l$-entry $l_v$ for which the invariant held before the union.
The changes in representatives of $x$ are reflected by $new\_left$ and $new\_right$, while the representative of $v$ is changed to $r(u)$.
The new entry for $l$ therefore respects the implication of the invariant.

To show the other implication, let $f(a,b) \in \mathcal{T}$.
The term $f(a,b)$ is entered via the \textt{addEquation} and subsequently via the \textt{addNode} method.
For compound terms lines \nllabel{ldefined} and \nllabel{initl} assert that $l$ is defined for $(r(a),r(b))$.
All changes to $r(a)$ or $r(b)$ must happen in union and they are reflected by matching updates to the $l$ structure.

\end{proof}


\begin{invariant}[Neighbours]

For every $s \in \mathcal{T}$, every $t_r \in rN(r(s))$ and $t_l \in lN(r(s))$, $l$ is defined for $(r(s),r(t_r))$ and $(r(t_l),r(s))$.

\end{invariant}

\begin{proof}

We show the result for the structure $rN$.
The result about $lN$ can be obtained similarly.
Since $rN$ is initialized with the empty set in line \ref{initrN} of addNode, the invariant clearly holds initially.
To show that the invariant always holds, it has to be shown that all modifications of $r$ and $rN$ do not change the invariant.
The structure $l$ is not modified after initialization.
The structure $r$ is modified in line \ref{changerep} of union.
The structure $rN$ is modified in line \ref{modifyrN} of addNode and line \ref{modifyrN2} of union.

Line \ref{modifyrN} of addNode adds $b$ to $rN(r(a))$ and the four lines before that addition show that $l$ is defined for $(r(a),r(b))$.

Union modifies $rN$ in such a way that it adds all right neighbors of some representative $r(v)$ to $rN(r(u))$.
Lines \ref{startrN} to \ref{stoprN} make sure that $l$ is defined for all these right neighbors.

\end{proof}

A consequence of this invariant is and the fact that the statement is true after inserting, that for every term $t \in \mathcal{T}$ of the form $f(a,b)$, $l$ is defined for $(r(a),r(b))$.

\begin{proposition}[Sound- \& Completeness]

Let $\mathcal{C}$ be the congruence structure obtained by adding equations $E = \langle (u_1,v_1), \ldots, (u_n,v_n) \rangle $ to the empty congruence structure.
For every $s,t \in \mathcal{T}$: $E \models s \thickapprox t$ if and only if $r(s) = r(t)$.

\end{proposition}

\begin{proof}

\textbf{Completeness}

We show that from $E \models s \thickapprox t$ follows $r(s) = r(t)$ by induction on $n$.

Base case $n=1$: $E \models s \thickapprox t$ implies either $s = t$ or $\{u_1,v_1\} = \{s,t\}$.
In the first case $r(s) = r(t)$ is trivial. 
In the second case, the claim follows from the fact that, when $(u_1,v_1)$ is entered, union is called with arguments $s$ and $t$.
After this operation $r(s) = r(t)$.

Induction hypothesis: For every sequence of equations $E_n$ with $n$ elements and every $s,t \in \mathcal{T}_{E_n}$: $E_n \models s \thickapprox t$ then $r(s) = r(t)$.

Induction step: Let $E = \langle (u_1,v_1), \ldots, (u_{n+1},v_{n+1}) \rangle$ and $E_n = \langle (u_1,v_1), \ldots, (u_n,v_n) \rangle$.
There are two cases: $E_n \models s\thickapprox t$ and $E_n \nvDash s\thickapprox t$.
In the former case, the claim follows from the induction hypothesis, the invariant class and the fact that union always changes representatives for all elements of a class.
We still have to show the claim in the latter case.
We write $E \models_n u \thickapprox v$ as an abbreviation for $E_n \nvDash u \thickapprox v$ and $E \models u \thickapprox v$.
We show the claim by induction on the structure of the terms $s$ and $t$.

Base case: $s$ or $t$ is a constant and therefore the transitivity reasoning was used to derive $E \models_n s \thickapprox t$.
In other words, there are $l$ terms $t_1,\ldots,t_l$ such that $s = t_1$, $t = t_l$ and for all $i = 1,\ldots,l-1: E \models_n t_i \thickapprox t_{i+1}$.
We prove by yet another induction on $l$ that $r(t_1) = r(t_l)$.
Base case $l = 2$. It has to be the case (up to swapping $u_{n+1}$ with $v_{n+1}$), that $E_n \models s \thickapprox u_{n+1}$ and $E_n \models t \thickapprox v_{n+1}$, and the outmost induction hypothesis implies $r(s) = r(u_{n+1})$ and $r(t) = r(v_{n+1})$.
Therefore it follows from Invariant Class, that after the call to union for $(u_{n+1},v_{n+1})$ it is the case that $r(t_1) = r(t_2)$.
Suppose that the claim holds for some $l \in \mathbb{N}$.
In the induction step, going from $l$ to $l+1$, the claim follows from a simple application of the transitivity axiom, since $t_1,\ldots,t_l$ and $t_2,\ldots,t_{l+1}$ are both sequences of length $l$.

For the induction step of the term-structure induction, suppose that $s = f(a,b)$ and $t = f(c,d)$.
There are two cases such that $E \models_n s \thickapprox t$ can be derived.
Using a transitivity chain, the claim can be shown just like in the base case.
Using the congruence axiom, it has to be the case that $E \models_n a \thickapprox c$ and $E \models_n b \thickapprox d$ (in fact one of those can also be the case without the $n$ index).
The terms $a,b,c,d$ are of lower structure than $s$ and $t$.
Therefore it follows from the induction hypothesis that $r(a) = r(c)$ and $r(b) = r(d)$.
The Invariants Neighbour and Lookup imply that either $r(s) = r(t)$ or $(s,t)$ is added to $d$ in line \ref{deduceEq1} or line line \ref{deduceEq2} of union.
Subsequently union is called for $s$ and $t$, after which $r(s) = r(t)$ holds.

\textbf{Soundness}

For $s = t$ the claim follows trivially.
Therefore we show soundness in case $s \neq t$.
We show that from $r(s) = r(t)$ follows $E \models s \thickapprox t$ by induction on the number $k$ of calls to union induced by adding all equations of $E$ to the empty congruence structure for all $s$ and $t$ that are arguments of some call to union.
The original claim then follows from invariant Class, since only union modifies the $r$ structure and the fact that two terms are in the same class if and only if union was called for some elements in the respective classes.
%Notice that only union modifies $r$ after initialization.
%Therefore the proof investigates the changes made by union and the parameters it is called when adding equations to a congruence structure.

Base case $k = 1$: $r(s) = r(t)$ implies $\{u_1,v_1\} = \{s,t\}$ and $E \models s \thickapprox t$ is trivial.
%Lines \ref{deduceEq1} and \ref{deduceEq2} do not induce any further unions, because no compound terms, that $s$ or $t$ could be a subterm of, are yet inserted in the congruence structure.

%Induction hypothesis: For every sequence of equations $E_n$ with $n$ elements and every $s,t \in \mathcal{T}_{E_n}$: $r(s) = r(t)$ after adding all $n$ equations to the empty congruence structure then $E_n \models s \thickapprox t$.

Induction hypothesis: For every $l < k$, if a set of equations $F$ induces $l$ calls to union, then from $r(s) = r(t)$ follows $F \models s \thickapprox t$ for all terms $s,t$ that are arguments of some call to union.

Induction step: Suppose $E = \langle (u_1,v_1), \ldots, (u_n,v_n) \rangle$ induces $k$ calls to union with arguments $(h_1,g_1),\ldots,(h_k,g_k)$.
The subsequence $E_n = \langle (u_1,v_1), \ldots, (u_{n-1},v_{n-1}) \rangle$ induced the first $l$ calls to union for some $n-1 \leq l < k$.
In other words, adding $(u_n,v_n)$ to the congruence structure induces the calls to union with arguments $(h_{k-l},g_{k-l}),\ldots,(h_k,g_k)$.
The first call to union with arguments $(h_{k-l},g_{k-l})$ is either an original input equation, or a deduced equality from line \ref{addnodemerge} of \texttt{addNode}.
In both cases $E \models h_{k-l} \thickapprox g_{k-l}$, which is trivial in the former case and an application of the induction hypothesis in the latter case.
Union induces additional union calls in such a way that the arguments of the additional call are on parent terms of the respective original arguments.
Therefore, using induction on the structure of terms, the original induction hypothesis, Invariants Lookup and Neighbour and lines \ref{startlN} to \ref{stoprN} of \texttt{union}, it can be shown that for all pairs $(h_m,g_m)$ and all $m = k-l+1,\ldots,k$ it is the case that $E \models h_m \thickapprox g_m$.

%
%We show the induction step by another induction on $k-l$, i.e. the number of unions induced by adding $(u_n,v_n)$ to the congruence structure.
%Base case $k-l = 1$: Union is called for $(u_n,v_n)$ and the induces no further calls union.
%After the one call to union $r(u_n) = r(v_n)$ and clearly $E \models u_n \thickapprox v_n$.
%The fact that lines \ref{deduceEq1} and \ref{deduceEq2} of union do not induce any further calls to the method, together with the induction hypothesis and the invariants Lookup and Neighbour, imply that $E \models_n s \thickapprox t$ if and only if $s,t = \{u_n,v_n\}$.
%
%Induction step: Suppose adding $(u_n,v_n)$ induces $m$ calls to union with arguments $(h_{k-m},g_{k-m}),\ldots,(h_k,g_k)$.
%The last call to union has to be added 
%
%Adding the equation $(u_{n+1},v_{n+1})$ to the working congruence structure induces a sequence of $l$ calls to union with parameters $(h_1,g_1),\ldots,(h_l,g_l)$

%After initialization of $r$ only union modifies the structure.
%Adding $E$ to the empty congruence structure induces a sequence of calls to union with parameters $(u_1,v_1), \ldots , (u_n,v_n)$.
%We show by induction on $n$, that $E \models v_i \thickapprox u_i$

\end{proof}

\begin{proposition}[Runtime]
\label{prop:runtime}
Let $E$ be a set of equations that uses $n$ terms.
Computing the congruence closure with our congruence closure algorithm takes worst-case time $O(n \log(n))$.

\end{proposition}

\begin{proof}

There are three loops in the method \texttt{union}, which are nested within the loop of \texttt{merge}.
These loops are clearly the dominating factor for runtime.

Lines \nlline{reverse1} and \nlline{reverse2} of \texttt{union} make sure that everytime the representative of a term is changed, the size of its congruence class is doubled.
The maximum size of a congruence class is $n$.
Therefore the representative of a single term is changed maximally $\log(n)$ times overall and line \ref{changerep} of \texttt{union} is not executed more than $n \log(n)$ times.

\end{proof}

\FloatBarrier

\subsection*{Congruence graph}

The main goal of this work is to replace redundant explanations with shorter ones.
For this purpose the input equations and deduced equalities have to be stored in a data structure that supports the production of explanations.
We support two different such data structures.
Both structures store equations in labeled graphs, which we call congruence graphs.
A node in such a graph represents a term and an edge between two nodes denotes that the represented terms are congruent w.r.t. the set of input equations.
A path in a congruence graph is a sequence of undirected, unweighted, labeled edges in the underlying graph.
The set of labels for both types of graphs is the set of extended equations $\mathcal{E}$.
Depending on the type of congruence graph used, it is not guaranteed that when \texttt{lazy\_insert} is called with arguments $s$ and $t$, there is an edge between $s$ and $t$.
However it is guaranteed that they are connected in the graph afterwards, i.e. there is a path between the nodes representing the terms.

\begin{invariant}[Paths]

For terms $s, t$ such that $s \neq t$ and a congruence structure with representative function $r$ holds $r(s) = r(t)$ if and only if there is a path in the congruence graph of the structure between $s$ and $t$

\end{invariant}

\begin{proof}

We show the claim by an induction on $|[r(s)]|$.
%We show that there is a path between $s$ and $t$ in the congruence graph induction on $|[r(s)]|$. 
The proof relies on the invariant Class, which shows the consistency between classes and representatives.

In the induction base $[r(s)] = \{s\}$, i.e. $r(s) = r(t)$ is false for every term $t \neq s$.
We have to show that there is no edge $(s,t)$ for $t \neq s$ in the congruence graph.
Edges are only added to the congruence graph via the \texttt{lazy\_insert} method which is only called in \texttt{merge}.
Clearly \texttt{merge} does not call \texttt{union} for $s$ and some term $t \neq s$, since otherwise $t \in [r(s)]$.
Therefore \texttt{merge} also does not add an edge for $s$ and some term $t \neq s$ to the congruence graph.

Let the induction hypothesis be, that for every term $s$ such that $|[r(s)]| \leq n$, for every term $t \neq s$ it is the case that $r(s) = r(t)$ if and only if there is a path between $s$ and $t$ in the congruence graph.

Suppose $[r(s)]$ is an arbitrary class with cardinality $n+1$.
Then there are two terms $u,v \in [r(s)]$ such that \texttt{union} was called for $u$ and $v$.
Before the union $|[r(u)]|$ and $|[r(v)]|$ both were strictly smaller than $n+1$.
In case they both belong to the same class before the union, the claim follows trivially by the induction hypothesis, since existing paths are not removed by adding new edges to the graph.
Suppose $s \in [r(u)]$ and $t \in [r(v)]$, then by induction hypothesis there are paths $p_1$ between $s$ and $u$ and $p_2$ between $t$ and $v$.
Right after the union of $u$ and $v$, an edge is inserted between them, so $p_1$ concatenated with $(u,v)$ and $p_2$ is a path between $s$ and $t$.
In case one of the terms did not belong to one of the classes before the union, it does not belong to the merged class after the union.
Also there was no path between the two terms before and since the only addition paths are between elements of $[r(u)]$ and $[r(v)]$, there is no path between the terms after the union.

\end{proof}

\begin{invariant}[Deduced Edges]

For every edge in a congruence structure between vertices $u,v$ with label $null$, 
there are $a,b,c,d \in \mathcal{T}$ such that $u = f(a,b)$, $v = f(c,d)$ and
there are paths in the underlying graphs between $a$ and $c$ aswell as $b$ and $d$.

\end{invariant}

\begin{proof}

Edges with label $null$ are added, when \texttt{merge} is called from \texttt{addNode}, or \texttt{union} induces an additional merge.
In both cases there are subterms with respective equal representatives.
The claim follows by using the invariant Paths.

\end{proof}

The method \texttt{explain} returns a path between its two arguments, if one exists.
Depending on the actual type of graph used, this path can be unique or not.
The method \texttt{inputEqs} for a path in the congruence graph returns the input equations that were used to derive the equality between the first and the last node of the path.
For an input equation, this is simply the equation itself.
For a deduced equality, this is the set of input equations that were used for deduction.
Combining these two methods, the statement \texttt{inputEqs(explain(s,t,g),g)} returns an explanation for $E \models s \thickapprox t$ if there is one.

\input{chapters/congruence/algorithms/inputeqs}

In the following, we describe the two types of congruence graphs we support.
They differ in the type of graph they use and how explanations are produced.

\subsubsection*{Equation Graph}

A equation graph stores input and deduced equalities in a labeled weighted undirected graph $(V,E)$ with 
$V \subseteq \mathcal{T}$, $E \subseteq V \times \mathcal{E} \times V \times \mathbb{N}$.
The weight for an edge is the number of input equalities used to derive the equality between its two nodes.
This number is one for input equalities and the size of the explanation for deduced equalities.
Edges are added to the graph, regardless whether the nodes are already connected in the graph.
Therefore there is a choice which path the explain method returns.
To produce short explanations, the shortest path w.r.t. the edge weights is returned.

Finding the shortest path between two nodes in a weighted graph is not trivial.
The single source shortest path problem (SSSP) is a classical graph problem in computer science.
The task is to find the shortest path in a graph between one designated node, the source, and all other nodes in the graph.
To the best knowledge of the authors, there is no algorithm to find the shortest path between two nodes which has better asymptotic runtime than one to solve SSSP.
There is a whole variety of algorithms that solve SSSP.
Classical algorithms for SSSP are those of Dijkstra \cite{Dijkstra1959} and Bellman-Ford \cite{Ford1956,Bellman1956}.
The algorithms work on different kinds of graphs.
Our setting is an undirected graph with positive integer weights.
We chose to use Dijkstra's algorithm, even though the algorithm does not have optimal asymptotic runtime.
It's worst-case runtime is $O(n \log(n))$ \cite{Cormen1989}, if the priority queue is implemented as a Fibonacci Heap, which is the case in our implementation.
\cite{Thorup1999} reports of an linear time algorithm for the undirected single source shortest path with positive integer weights problem.
However, the algorithm has a big overhead and needs several precomputations.
\cite{Cherkassky1996} is an extensive study of several shortest path algorithms which shows that Dijkstra's algorithm performs well in practice.

Dijkstra's algorithm finds shortest paths to an increasing set of nodes, until every node has been discovered.
It does so by keeping track of the the shortest paths and the distances, being the combined weights of edges on the path, of nodes to the source.
Initially, the only discovered node is the source itself and the distance to every other node is infinite.
The algorithm discovers new nodes by selecting the lowest weight outgoing edge of all nodes that have been discovered so far and updates shortest paths and distances while doing so.
It is a greedy algorithm in the sense that it always locally chooses lowest weight edges and never discards previously made decisions.

The algorithm has been slightly modified to take into account decisions that are edges for deduced equalities.
These edges represent explanations, which are a sets of input equations.
Previously included input equations do not increase the size of the global explanation when including them again.
Therefore the modified Dijkstra algorithm adds an edge with weight 0 for every input equation in the explanation of a deduced equality edge.
This is done to reduce the size of explanations.
Since previous decisions are not discarded, it is not guaranteed that the modified algorithm returns the shortest path in the final graph, including the extra edges.
Example \ref{ex:short_expl} demonstrates that the modified shortest path algorithm does not always produce the shortest explanation, but can produce shorter explanations than the unmodified version in some situations.
The shortest path algorithm's inability to return shortest explanations is not surprising, since it runs in $O(n \log(n))$ and in Section \ref{sec:sec:npcomplete} it was shown that finding the shortest explanation is NP-complete.

\begin{example}
Consider the congruence graph shown in Figure \ref{fig:short_expl}, where solid edges are input equation and the dashed edge marks an application of the congruence axiom.
The equality of $f(c_1,e)$ and $f(c_4,e)$ was deduced using the equations $(c_1,c_2),(c_2,c_3),(c_3,c_4)$, which is the shortest path in the graph between $c_1$ and $c_4$, obtained from a previous call to the shortest path algorithm.

Suppose we want to compute an explanation for $a \thickapprox b$.
Clearly the input equalities $(a,f(c_1,e))$, $(f(c_4,e),c_1)$ and the explanation for $f(c_1,e) \thickapprox f(c_4,e)$ have to be included in the explanation.
Additionally $c_1 \thickapprox b$ has to be explained.
For this equality the set $(c_1,d_1),(d_1,d_2),(d_2,b)$ is the shortest explanation in the original graph.
This sub explanation adds three new equations to the explanation for $a \thickapprox b$.
Therefore when the shortest path algorithm iterates over the edge $(f(c_1,e),f(c_4,e))$, it can add add zero weight edges $(c_1,c_2),(c_2,c_3),(c_3,c_4)$ to the graph.
By doing so the shortest explanation for $c_1 \thickapprox b$ becomes $(c_1,c_2),(c_2,c_3),(c_3,c_4),(c_4,b)$, which only adds one extra equation to the global explanation.

This method is successful in finding the shortest explanation in this example if the search begins in the node $a$.
Should the search begin in the node $b$, the edges including $d_1$, $d_2$ are added to the shortest path before the edge $(f(c_1,e),f(c_4,e))$ is touched.
Therefore the undesired long explanation would be returned.

\begin{figure}[!h]
\input{chapters/congruence/figures/dijkstra}
\caption{Short explanation example}
\label{fig:short_expl}
\end{figure}

\label{ex:short_expl}
\end{example}

\input{chapters/congruence/algorithms/insert_dij}

\input{chapters/congruence/algorithms/explain_dij}

\FloatBarrier

\subsubsection*{Proof Forest}

A proof forest is a collection of proof trees.
A proof tree is a labeled tree with nodes in $\mathcal{T}$ and edge labels in $\mathcal{E}$.
For every congruence class in the current status of a congruence structure, there is one proof tree.
Inserting an edge between nodes $s$ and $t$ of different proof trees is done by making one the child of the other.
To maintain a tree structure, all edges between the new child and the root of its tree are reversed.
To limit the number of edge revering steps, the smaller tree is always attached to the bigger one.
This results in $O(n \log(n))$ edge reversing steps, where $n$ is the number terms in the input equation set.
This bound can be shown using the same argument as in the proof of Proposition \ref{prop:runtime}.
As stated earlier, we understand a path as a sequence of undirected edges.
In case of a proof tree, a path between $s$ and $t$ of the same tree is the combined sequence of edges between the nodes and their nearest common ancestors.
The structure, up to small changes, was proposed by \cite{Nieuwenhuis2005,Nieuwenhuis2007}.
Its benefit is the quick access of explanations and good overall runtime.
Its downside is its inflexibility when it comes to producing alternative explanations.
In fact the explanation returned is always the first one to occur during edge insertion.
The authors improve the structure for the special case of flattened terms, for which no term has nesting depth greater than one.

\input{chapters/congruence/algorithms/insert_pt}

\input{chapters/congruence/algorithms/explain_pt}

\begin{example}

Add equations from example \ref{ex:short_expl} with d - terms first into structure and show why it produces longer proof.

\end{example}

%See how BarceLogic ppl prove stuff,
%-) tree is still tree after inserting
%-) path to NCA forms explanation

\FloatBarrier

\subsection*{Proof Production}

In this section we describe how to produce resolution proofs from paths in a congruence graph.
The method to carry out this operation is \texttt{prodcureProof}.
The basic idea is to traverse the path, creating a transitivity chain of equalities between adjacent nodes, while keeping track of the deduced equalities in the chain.
From invariant Deduced Edges follows that for the deduced equalities there have to be paths between the respective arguments of the compound terms.
These paths are transformed into proof recursively and resolved with a suiting instance of the congruence axiom.
Afterwards the subproof is resolved with the original transitivity chain.
Since terms can never be equal to their subterms, the procedure will eventually terminate.
The result of this procedure is a resolution proof with a root, such that the equations of the negative literals are an explanation of the target equality.
In other words, let $s \thickapprox t$ be the equality to be explained and suppose \texttt{produceProof} returns a proof with root $\rho$.
Then for $\rho$ it is the case that $F := \{(u,v) \mid u \neq v \text{ is a literal in } \rho\} \models s \thickapprox t$ and $F$ is a subset of the input equations.

\input{chapters/congruence/algorithms/prodproof}


\begin{example}

Consider again the congruence graph shown in Figure \ref{fig:short_expl} and suppose we want a proof for $a \thickapprox b$.
Suppose we found the path $p_1 := \langle  a, f(c_1,e), f(c_4,e), c_1, c_2, c_3, c_4, b \rangle$ as an explanation and that the explanation for $f(c_1,e) \thickapprox f(c_4,e)$ is the path $\langle c_1, c_2, c_3, c_4 \rangle$.
We transform $p_1$ and $p_2$ into instances of the transitivity axiom $C_1$ and $C_2$ respectively. 
The clause $C_2$ is resolved with the instance of the congruence axiom $C_3$, which is then resolved with the instance of the reflexive axiom $C_4$ resulting in clause $C_5$.
Finally, $C_1$ is resolved with $C_5$ to obtain the final clause $C_6$ and after resolving with all input equations, we obtain $C_7$.

\begin{align*}
C_1 &:= \{a \neq f(c_1,e), f(c_1,e) \neq f(c_4,e), f(c_4,e) \neq c_1, c_1 \neq c_2, c_2 \neq c_3, c_3 \neq c_4, c_4 \neq b, a = b\} \\
C_2 &:= \{c_1 \neq c_2, c_2 \neq c_3, c_3 \neq c_4, c_1 = c_4\} \\
C_3 &:= \{e \neq e, c_1 \neq c_4, f(c_1,e) = f(c_4,e)\} \\
C_4 &:= \{e = e\} \\
C_5 &:= \{c_1 \neq c_2, c_2 \neq c_3, c_3 \neq c_4, f(c_1,e) = f(c_4,e)\} \\
C_6 &:= \{a \neq f(c_1,e), f(c_4,e) \neq c_1, c_1 \neq c_2, c_2 \neq c_3, c_3 \neq c_4, c_4 \neq b, a = b\} \\
C_7 &:= \{a = b\} 
\end{align*}

\end{example}

\FloatBarrier

\subsection*{Congruence Compressor}

In Section \ref{TODO} processing of a proof was defined.
The most important application of proof processing for this work is proof compression.
We want to make use of the short explanations found by the congruence closure algorithm described above.
To this end we replace subproofs with new proofs that have shorter conclusions.
Shorter conclusions lead to less resolution steps further down the proof.
There is however a tradeoff in overall proof length when introducing new subproofs.
The subproof with a shorter conclusion can still be longer, i.e. involve more resolution nodes, than one with a bigger conclusion.

\begin{example}
Consider the set of equations $\{(f(a,a),a),(a,b),(b,f(b,b))\}$ and the target equality $f(a,a) \thickapprox f(b,b)$.
Both the instance of the transitivity axiom $\{f(a,a) \neq a, a \neq b, b \neq f(b,b), f(a,a) = f(b,b)\}$ and the clause $\{a \neq b, f(a,a) = f(b,b)\}$ or one congruence axiom resolved against the i

\end{example}

The Congruence Compressor does exactly this. It is defined upon the following processing function, specified in pseudocode.
Input equations are assumed to be true, i.e. an input equation $(u,v)$ is treated as an axiomatic clause $\{u = v\}$.
In a last step of \texttt{prodProof} all input equations are resolved away.

\input{chapters/congruence/algorithms/compressor}

The compressor (Algorithm \ref{algo:compressor}) uses the method \texttt{fixNode} to maintain a correct proof.
The method modifies nodes with premises that have earlier been replaced by the compressor. 
Nodes with unchanged premises are not changed.
Let $n$ be a proof node that was derived by resolving $pr_1$ and $pr_2$ using pivot $\ell$.
It assumed that the values $pr_1$, $pr_2$ and $\ell$ are stored together with the node and can be accessed in constant time.
Note that the method returns $p_1$ in case non of the new premises contains the pivot.
We might as well choose $p_2$ to maintain obtain a correct node.

\input{chapters/congruence/algorithms/fixNode}

\FloatBarrier