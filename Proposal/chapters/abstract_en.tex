\chapter*{Abstract}

\section{Problem definition}

Proofs are the backbone of mathematics. 
They allow scientists to build theorems on top of others theorems and thus discover new knowledge.
But proofs not only serve as assurance, they can also provide insight on the nature of the underlying problem.\\
Both these statements are also true for formal proofs, which are particularly interesting to Computer Science.\\
A formal proof enables one system to trust the output of another and therefore systems can safely be built on top of another. 
For example SAT-Solvers are used extensively in modern deductive systems \cite{TODO}. 
However these solvers may contain bugs, therefore it may be dangerous to blindly trust the output of it.
A formal proof can assure the correctness of the output.\\
On the other hand from formal proofs, information can be extracted, which can be used to solve another problem.
For example interpolants \cite{TODO}, which have important applications in Software Verification, or Unsat-cores \cite{TODO}, TODO: What are they good for?, can be extracted from formal proofs.\\

Typically problems that are tackled by automated systems are large. 
So large that even for algorithms with low complexity, it is highly desirable to reduce the hardness of the input, while assuring the same quality of output.\\
This is where our work comes into play.
We present methods to compress formal proofs, produced by SMT- or SAT- Solvers, along two different measures.\\

The first technique aims at reducing the length of proofs, by removing redundancies in the congruence closure part of an SMT-proof.
Given a set of equalities of terms, a congruence closure procedure builds classes of terms, that are proven to be equal, when assuming the following four axioms: \emph{reflexivity}, \emph{symmetry}, \emph{transitivity} and \emph{congruence} (two compound terms are equal if they have the same arity, use the same function symbols and all their arguments are equal).\\
We found that state of the art SMT-Solvers produce needlessly large sets of equalities as proof of the congruence of two terms 
and our goal is to find smaller such sets.
Smaller explanations not only lead to a smaller proofs of the congruence itself, 
but also decrease the number of inference steps in a proof using the congruence for reasoning.\\

The other part of this Thesis is devoted to memory consumption of proof processing.\\
Proof precessing could be checking the correctness, extracting information like interpolants or proof manipulating techniques like the above.\\
Usually a proof is explicitly or implicitly represented as a directed acyclic graph.
To process it, its nodes have to be traversed and therefore ordered.
The procedures usually require that nodes are traversed in an order, such that all parent- (Top-Down) or that all children- (Bottom-Up) nodes are visited before the node itself.
For example, to check the correctness of one node, the correctness of its parent nodes is essential. 
We refer to orders for Top-Down traversals as \emph{topological orders}. Note that orders for Top-Down and Bottom-Up traversals are dual in the sense that the reverse of a Top-Down traversal order is a Bottom-Up traversal order.\\
The key insight towards space compression is that during proof processing, not all nodes have to be kept in memory at all times.\\
In a Top-Down traversal, a node can be dropped from as soon as its last children has been visited, because the original node will not be accessed anymore.\\
For a given proof, there are at least exponentially many topological orders and the problem of finding an optimal order w.r.t. space is NP-complete \cite{TODO} for orders, which visit each node only once, and PSPACE-complete \cite{TODO} for orders, which visit nodes more than once.\\
This work presents algorithms to construct topological orders using heuristics.

%\begin{itemize}
	%\item Reflexivity: For all terms $t$: $t = t$
	%\item Symmetry: For all terms $t$ and $s$: $t = s$ \emph{iff} $s = t$
	%\item Transitivity: For all terms $t_1,t_2,t_3$: $t_1 = t_2$ and $t_2 = t_3$ implies $t_1 = t_3$
	%\item Congruence: For all terms $t = f(t_1\ldots t_n)$ and $s = f(t'_1 \ldots t'_n)$: For all $i$: $t_i = t'_i$ implies $t = s$
%\end{itemize}

\section{Expected results}

We will propose algorithms for compressing proofs and show their complexity.\\
The space compression algorithms will be completely new and they will be useable for other problems using directed acyclic graphs with a unique sink.\\
The congruence closure algorithm will be an adaption of Pascal Fontaine's algorithm \cite{TODO: Pascal's Thesis}, possibly using ideas from \cite{TODO: BarceLogic CC algorithm}. 
Possibly our algorithm will be able to obtain shorter explanations in general than the two explanation producing algorithms proposed earlier.\\
The question whether for a given equality there exists an explanation of $k$ equations is believed to be NP-complete \cite{TODO: BarceLogic CC algorithm}.
However, to the best of our knowledge, this result has not yet been proven in any publication.
We will try to fill this gap by either proving or disproving the claim.\\
We will implement algorithms and run experiments on a broad range of proofs, produced by state of the art SAT- and SMT- Solvers.
Therefore we will demonstrate the actual compression achieved.

\section{Methodology and approach}

The algorithms will be implemented into the proof compression software Skeptik \cite{TODO: Skeptik system description}.
The experiments will be run on the Vienna Scientific Cluster.

\section{State of the art}

The research field of proof complexity studies lower bounds of various measures of proofs in different proof calculi \cite{TODO: modern approach to complexity?}. A typical questions of this fields intuitively is of the following kind: Given a proof calculus $\mathcal{C}$ and a problem $p \in \mathcal{P}$, where $\mathcal{P}$ is a class of problems, how big is the shortest proof of $p$ in $\mathcal{C}$.
One classical result is the exponential length of resolution refutations in the propositional resolution calculus \cite{TODO: modern approach to complexity?}.\\
Besides the classical length measure, space requirements of proofs have been studied \cite{TODO: JAKOB NORDSTROM paper} using pebbling games.\\
Our approach differs from this field of study, as we do not mean to prove new lower bounds for a class of problems, but to provide concrete algorithms to reduce measures of given proofs.

For the propositional resolution calculus many length compression algorithms have been proposed \cite{TODO: quote all the algorithms}.\\
To the best of our knowledge there no algorithm to compress proofs in space, nor an algorithm to construct strategies in pebbling games, has been proposed so far.\\
Congruence closure has been long studied and classical algorithms have been proposed by Nelson and Oppen \cite{TODO: Nelson-Oppen paper}, Downey, Sethi and Tarjan \cite{TODO: DST Paper} and Shostak Paper \cite{TODO: Shostak paper}. %Possibly say something about it's important applications
More recently abstract congruence closure has been proposed \cite{TODO:Abstract Congruence Closure and Specializations paper}, which replaces subterms with fresh constants to simplify the congruence closure algorithm.\\
Congruence closure algorithms producing explanations have been proposed by Pascal Fontaine \cite{TODO: Pascals Thesis} and Nieuwenhuis-Oliveras \cite{TODO: BarceLogic CC paper}, which uses the ideas of abstract congruence closure.\\
Our approach is closer to Fontaine's algorithm, because we want to reduce the number of literals and therefore do not want to introduce new constants.
Even though the new constants can be removed from explanations, in our setting the algorithm will be applied many times to small instances and therefore we think that the overhead of dealing with the extra constants is not worth it.