\chapter*{Abstract}

\section{Problem definition}

Proofs are the backbone of mathematics. 
They allow scientists to build theorems on top of other theorems and thus discover new knowledge.
But proofs not only serve as assurance or stepping stones, they can also provide insight on the nature of the underlying problem.\\
Both these statements are also true for formal proofs, which are particularly interesting to Computer Science.\\
A formal proof enables one system to trust the output of another and therefore systems can safely be built on top of another. 
For example SAT-Solvers are used extensively in modern deductive systems \cite{TODO}. 
However these solvers may contain bugs, therefore it may be dangerous to blindly trust the output of it.
A formal proof can assure the correctness of the output.\\
On the other hand from formal proofs, information can be extracted, which can be used to solve related problems.
For example interpolants \cite{TODO}, which have important applications in Software Verification, or Unsat-cores \cite{TODO}, TODO: What are they good for?, can be extracted from formal proofs.\\

Typically problems that are tackled by automated systems are large. 
So large that even for algorithms with low complexity, it is highly desirable to reduce the hardness of the input, while assuring the same quality of output.\\
This is where our work comes into play.
We present methods to compress formal proofs, produced by SMT- or SAT- Solvers, along two different measures.\\

The first technique aims at reducing the length of proofs, by removing redundancies in the congruence closure part of SMT-proofs.
Given a set of equalities of terms, a congruence closure procedure builds classes of terms, that are proven to be equal, when assuming the following four axioms: \emph{reflexivity}, \emph{symmetry}, \emph{transitivity} and \emph{congruence} (two compound terms are equal if they have the same arity, use the same function symbols and all their arguments are equal).\\
We found that state of the art SMT-Solvers often produce needlessly large sets of equalities as proof of the congruence of two terms 
and our goal is to find smaller such sets.
Smaller explanations not only lead to a smaller proofs of the congruence itself, 
but also decrease the number of inference steps in a proof using the congruence for reasoning.\\

The other part of this Thesis is devoted to memory consumption of proof processing.\\
Proof precessing could be checking the correctness, extracting information like interpolants or proof manipulating techniques like the above.\\
Usually a proof is explicitly or implicitly represented as a directed acyclic graph.
To process it, its nodes have to be traversed and therefore ordered.
The procedures usually require that nodes are traversed in an order, such that all parent- (Top-Down) or that all children- (Bottom-Up) nodes are visited before the node itself.
For example, to check the correctness of one node, the correctness of its parent nodes is essential. 
We refer to orders for Top-Down traversals as \emph{topological orders}. Note that orders for Top-Down and Bottom-Up traversals are dual in the sense that the reverse of a Top-Down traversal order is a Bottom-Up traversal order.\\
The key insight towards space compression is that during proof processing, not all nodes have to be kept in memory at all times.\\
In a Top-Down traversal, a node can be dropped from memory as soon as its last children, w.r.t. the used order, has been visited, because the original node will not be accessed anymore.
Such deletion information can be stored added to the proof formats to indicate when nodes can safely be removed from memory.\\
For a given proof, there are at least exponentially many topological orders and the problem of finding an optimal order w.r.t. space is NP-complete \cite{Sethi1975} for orders, which visit each node only once, and PSPACE-complete \cite{Gilbert1980} for orders, which visit nodes more than once.\\
This work presents algorithms to construct topological orders using heuristics.

%\begin{itemize}
	%\item Reflexivity: For all terms $t$: $t = t$
	%\item Symmetry: For all terms $t$ and $s$: $t = s$ \emph{iff} $s = t$
	%\item Transitivity: For all terms $t_1,t_2,t_3$: $t_1 = t_2$ and $t_2 = t_3$ implies $t_1 = t_3$
	%\item Congruence: For all terms $t = f(t_1\ldots t_n)$ and $s = f(t'_1 \ldots t'_n)$: For all $i$: $t_i = t'_i$ implies $t = s$
%\end{itemize}

\section{Expected results}

We will propose algorithms for compressing proofs and show their complexity.\\
The space compression algorithms will be completely new and they will be useable for other problems using directed acyclic graphs with a unique sink.\\
The congruence closure algorithm will be an adaption of Pascal Fontaine's algorithm \cite{Fontaine2004}, possibly mixed with ideas from \cite{TODO: BarceLogic CC algorithm}. 
Hopefully our algorithm will be able to obtain shorter explanations (in the number of equations) in general than the two explanation producing algorithms proposed earlier.\\
The question whether for a given equality there exists an explanation of $k$ equations is believed to be NP-complete, as claimed in \cite{Nieuwenhuis2005a,Nieuwenhuis2007}.
%However, to the best of our knowledge, this result has not yet been proven in any publication.
However the authors do not know of a publication that has proven this question yet.
We will try to fill this gap by either proving or disproving the claim.\\
We will implement algorithms and run experiments on a broad range of proofs, produced by state of the art SAT- and SMT- Solvers.
Thus we will demonstrate the actual compression achieved.

\section{Methodology and approach}

The algorithms will be implemented into the proof compression software Skeptik \cite{Boudou} in the programming language Scala.
The experiments will be carried out on the Vienna Scientific Cluster.
Theorems stating the complexity of our algorithms will have to be proven from scratch or adapted from earlier publications.
To show the NP-completeness of the shortest explanation problem, two things have to be done. 
First we need to find an algorithm that, given an Oracle to make correct decisions, finds an explanation with $k$ or less equations in polynomial time and second we need to reduce another NP-complete problem to the shortest explanation problem.
The Palest Path Problem \cite{Tiwari} is one option for reduction, that has been foreseen to possibly be fruitful by the source of the claim in \cite{Nieuwenhuis2005a,Nieuwenhuis2007}.

\section{State of the art}

The research field of proof complexity studies lower bounds of various measures of proofs in different proof calculi \cite{Arora2009}. A typical questions of this fields intuitively is of the following kind: Given a proof calculus $\mathcal{C}$ and a problem $p \in \mathcal{P}$, where $\mathcal{P}$ is a class of problems, what is the worst case lower bound of some measure $m(.)$ of proofs, of all problems $p \in \mathcal{P}$ and their proofs in $\mathcal{C}$. % among all proofs in $\mathcal{C}$ of a problem $p$ or a problem class $\mathcal{P}$.

One classical result is the worst case exponential length of resolution refutations in the propositional resolution calculus \cite{Arora2009}.\\
Besides the classical length measure, space requirements of proofs have been studied \cite{Sethi1975,Nordstrom2013,Hopcroft1977,Esteban2001,Ben-Sasson2002} using pebbling games.\\
Our approach differs from this field of study, as we do not mean to prove new lower bounds for a classes of problems or calculi, but to provide concrete algorithms to reduce measures of given proofs.

For the propositional resolution calculus many length compression algorithms have been proposed \cite{Bar-Ilan2009,Boudou2013,Fontaine2011a,Cotton2010}. 
On the other hand, for First Order Logic Cut-Introduction has been studied \cite{Hetzl2012}, which is a form of proof compression. However none of the approaches tackle the congruence reasoning of proofs.\\
To the best of our knowledge neither an algorithm to compress proofs in space, nor one to construct strategies in pebbling games, has been proposed so far. 
The DRUP proof format \cite{Heule} is an extension of the well known RUP format, with the addition of deletion information.
The format has been introduced to help cope with huge proofs produced by solvers such as the ?GB proof documented in \cite{TODO: the big proof thing}.
Skeptik's native proof output format has already been enriched by optional deletion information.\\
Congruence closure has been long studied and classical algorithms are those of Nelson and Oppen \cite{Nelson1980}, Downey, Sethi and Tarjan \cite{Downey1980} and Shostak \cite{Shostak1978}. %Possibly say something about it's important applications
More recently abstract congruence closure has been proposed \cite{Bachmair2000}, which replaces subterms with fresh constants to simplify the congruence closure algorithm.\\
Congruence closure algorithms producing explanations have been proposed by Pascal Fontaine \cite{Fontaine2004} and Nieuwenhuis-Oliveras \cite{Nieuwenhuis2005a,Nieuwenhuis2007}, which uses the ideas of abstract congruence closure.\\
Our approach is closer to Fontaine's algorithm, because we want to reduce the number of literals and therefore do not want to introduce new constants.
Even though the new constants can be removed from explanations, in our setting the algorithm will be applied many times to small instances and not the other way around. 
Additionally Pascal Fontaine has shown that his method has the same worst case complexity as Nieuwenhuis-Oliveras' version. Therefore we think that the overhead of dealing with the extra constants is not worth the possible performance edge \cite{Bachmair2000} that abstract congruence closure comes with when looking at single instances.